{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc18596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d7d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avatar_on_image(base,emoji):\n",
    "    base_image = np.array(base)\n",
    "    emoji = np.array(emoji)\n",
    "    if emoji.shape[2]==4:\n",
    "        emoji = emoji[:,:,:3]\n",
    "    \n",
    "    (rows,cols,cha) = base_image.shape\n",
    "    (r,c,ch) = emoji.shape\n",
    "    \n",
    "    emoji = cv2.resize(emoji,(180,200))\n",
    "    base_image[:200, :180] = emoji[:,:]\n",
    "    base_image = base_image.astype(np.uint8)\n",
    "    \n",
    "    return base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8161f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# load the video file; if video from cam replace file name with 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "emotion_dic = {0:'Angry', 1:'Happy', 2:'Neutral', 3:'Sad'}\n",
    "emotion_dic = ('Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprised', 'Neutral')\n",
    "\n",
    "path0 = r'emojis/female/'\n",
    "path1 = r'emojis/male/'\n",
    "# female_emoji_dic = {0:path0+'angry.png', 1:path0+'happy.png', 2:path0+'neutral.png', 3:path0+'sad.png'}\n",
    "# male_emoji_dic = {0:path1+'angry.png', 1:path1+'happy.png', 2:path1+'neutral.png', 3:path1+'sad.png'}\n",
    "\n",
    "female_emoji_dic = {0:path0+'angry.png', 1:path0+'disgust.png', 2:path0+'fear.png', 3:path0+'happy.png', 4:path0+'neutral.png', 5:path0+'sad.png', 6:path0+'surprise.png'}\n",
    "male_emoji_dic = {0:path1+'angry.png', 1:path1+'disgust.png', 2:path1+'fear.png', 3:path1+'happy.png', 4:path1+'neutral.png', 5:path1+'sad.png', 6:path1+'surprise.png'}\n",
    "\n",
    "\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Load and Initialize the  classifiers required\n",
    "face_detector = cv2.CascadeClassifier('C:/Users/Vijay/Desktop/Jo/Applied_AI_Assignments/Emotion_detection_project/models/haarcascade_frontalface_default.xml')\n",
    "emo_model = tf.keras.models.load_model('models/emotion_detection.h5')\n",
    "gen_model = tf.keras.models.load_model('models/gen_model_94.h5')\n",
    "\n",
    "\n",
    "# Read until video is completed\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)         # converts video frame into gray frame\n",
    "        faces = face_detector.detectMultiScale(gray, 1.3, 5)   # detects four co-ordinates of the fa\n",
    "     \n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 2) # create box using co-ordinates\n",
    "            roi_frame = gray[y:y+h, x:x+w]\n",
    "            resized_roi = cv2.resize(roi_frame, (48,48), interpolation = cv2.INTER_LINEAR)\n",
    "            res = tf.expand_dims(resized_roi, 2)\n",
    "            res1 = tf.expand_dims(res, 0)\n",
    "\n",
    "            emo_predict = emo_model.predict(res1)              # it predicts the emotion from frame\n",
    "            gen_predict = gen_model.predict(res1)              # it predicts the gender from frame\n",
    "\n",
    "            maxindex_emo = int(np.argmax(emo_predict))         # get the maxindex from model prediction\n",
    "            maxindex_gen = int(np.argmax(gen_predict))         # get the maxindex from model prediction\n",
    "            \n",
    "            # put text above the box\n",
    "            cv2.putText(frame, emotion_dic[maxindex_emo],(x+20, y-60),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2,cv2.LINE_AA)\n",
    "#             cv2.imshow('Frame',frame)\n",
    "#             emoji creation on the frame\n",
    "            if maxindex_gen==0:\n",
    "                emoji0 = cv2.imread(female_emoji_dic[maxindex_emo])\n",
    "                frame = avatar_on_image(frame,emoji0) \n",
    "                cv2.imshow('Frame',frame)\n",
    "            elif maxindex_gen==1:\n",
    "                emoji1 = cv2.imread(male_emoji_dic[maxindex_emo])\n",
    "                frame = avatar_on_image(frame,emoji1)\n",
    "                cv2.imshow('Frame',frame)\n",
    "    \n",
    "    \n",
    "        # Press Q on keyboard to  exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()             # when everything is done, release the video\n",
    "cv2.destroyAllWindows()   # close all the windows opeded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
